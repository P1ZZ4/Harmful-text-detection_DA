2021-05-15 08:33:04.809733: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
=== Argument Setting ===
src: news
tgt: community
seed: 42
train_seed: 42
model_type: kobert
max_seq_length: 128
batch_size: 32
pre_epochs: 3
num_epochs: 3
AD weight: 1.0
KD weight: 1.0
temperature: 20
Downloading: 100% 77.8k/77.8k [00:00<00:00, 583kB/s]
Downloading: 100% 51.0/51.0 [00:00<00:00, 60.3kB/s]
=== Processing datasets ===
writing example 200 of 800
writing example 400 of 800
writing example 600 of 800
writing example 800 of 800
writing example 200 of 200
writing example 200 of 1000
writing example 400 of 1000
writing example 600 of 1000
writing example 800 of 1000
writing example 1000 of 1000
writing example 200 of 800
writing example 400 of 800
writing example 600 of 800
writing example 800 of 800
Downloading: 100% 426/426 [00:00<00:00, 367kB/s]
Downloading: 100% 369M/369M [00:06<00:00, 57.9MB/s]
=== Training classifier for source domain ===
Epoch [01/03] Step [001/025]: cls_loss=0.7070
Epoch [01/03] Step [002/025]: cls_loss=0.6859
Epoch [01/03] Step [003/025]: cls_loss=0.6876
Epoch [01/03] Step [004/025]: cls_loss=0.7162
Epoch [01/03] Step [005/025]: cls_loss=0.6840
Epoch [01/03] Step [006/025]: cls_loss=0.7303
Epoch [01/03] Step [007/025]: cls_loss=0.6855
Epoch [01/03] Step [008/025]: cls_loss=0.7270
Epoch [01/03] Step [009/025]: cls_loss=0.6897
Epoch [01/03] Step [010/025]: cls_loss=0.7228
Epoch [01/03] Step [011/025]: cls_loss=0.7377
Epoch [01/03] Step [012/025]: cls_loss=0.6882
Epoch [01/03] Step [013/025]: cls_loss=0.6897
Epoch [01/03] Step [014/025]: cls_loss=0.7326
Epoch [01/03] Step [015/025]: cls_loss=0.7188
Epoch [01/03] Step [016/025]: cls_loss=0.7343
Epoch [01/03] Step [017/025]: cls_loss=0.7553
Epoch [01/03] Step [018/025]: cls_loss=0.6998
Epoch [01/03] Step [019/025]: cls_loss=0.6797
Epoch [01/03] Step [020/025]: cls_loss=0.7027
Epoch [01/03] Step [021/025]: cls_loss=0.7005
Epoch [01/03] Step [022/025]: cls_loss=0.7158
Epoch [01/03] Step [023/025]: cls_loss=0.7142
Epoch [01/03] Step [024/025]: cls_loss=0.7089
Epoch [01/03] Step [025/025]: cls_loss=0.6927
Epoch [02/03] Step [001/025]: cls_loss=0.6954
Epoch [02/03] Step [002/025]: cls_loss=0.6795
Epoch [02/03] Step [003/025]: cls_loss=0.6583
Epoch [02/03] Step [004/025]: cls_loss=0.6852
Epoch [02/03] Step [005/025]: cls_loss=0.7240
Epoch [02/03] Step [006/025]: cls_loss=0.6911
Epoch [02/03] Step [007/025]: cls_loss=0.7104
Epoch [02/03] Step [008/025]: cls_loss=0.6550
Epoch [02/03] Step [009/025]: cls_loss=0.6989
Epoch [02/03] Step [010/025]: cls_loss=0.6880
Epoch [02/03] Step [011/025]: cls_loss=0.6998
Epoch [02/03] Step [012/025]: cls_loss=0.6931
Epoch [02/03] Step [013/025]: cls_loss=0.6891
Epoch [02/03] Step [014/025]: cls_loss=0.7090
Epoch [02/03] Step [015/025]: cls_loss=0.7081
Epoch [02/03] Step [016/025]: cls_loss=0.6840
Epoch [02/03] Step [017/025]: cls_loss=0.6953
Epoch [02/03] Step [018/025]: cls_loss=0.6980
Epoch [02/03] Step [019/025]: cls_loss=0.7004
Epoch [02/03] Step [020/025]: cls_loss=0.6755
Epoch [02/03] Step [021/025]: cls_loss=0.6957
Epoch [02/03] Step [022/025]: cls_loss=0.7024
Epoch [02/03] Step [023/025]: cls_loss=0.6825
Epoch [02/03] Step [024/025]: cls_loss=0.6794
Epoch [02/03] Step [025/025]: cls_loss=0.7045
Epoch [03/03] Step [001/025]: cls_loss=0.6880
Epoch [03/03] Step [002/025]: cls_loss=0.6702
Epoch [03/03] Step [003/025]: cls_loss=0.7290
Epoch [03/03] Step [004/025]: cls_loss=0.7175
Epoch [03/03] Step [005/025]: cls_loss=0.7080
Epoch [03/03] Step [006/025]: cls_loss=0.6673
Epoch [03/03] Step [007/025]: cls_loss=0.6871
Epoch [03/03] Step [008/025]: cls_loss=0.6816
Epoch [03/03] Step [009/025]: cls_loss=0.7110
Epoch [03/03] Step [010/025]: cls_loss=0.7372
Epoch [03/03] Step [011/025]: cls_loss=0.6906
Epoch [03/03] Step [012/025]: cls_loss=0.6952
Epoch [03/03] Step [013/025]: cls_loss=0.7175
Epoch [03/03] Step [014/025]: cls_loss=0.6756
Epoch [03/03] Step [015/025]: cls_loss=0.7134
Epoch [03/03] Step [016/025]: cls_loss=0.6714
Epoch [03/03] Step [017/025]: cls_loss=0.6758
Epoch [03/03] Step [018/025]: cls_loss=0.7171
Epoch [03/03] Step [019/025]: cls_loss=0.6725
Epoch [03/03] Step [020/025]: cls_loss=0.6764
Epoch [03/03] Step [021/025]: cls_loss=0.7189
Epoch [03/03] Step [022/025]: cls_loss=0.7104
Epoch [03/03] Step [023/025]: cls_loss=0.6933
Epoch [03/03] Step [024/025]: cls_loss=0.6864
Epoch [03/03] Step [025/025]: cls_loss=0.6689
save pretrained model to: /content/snapshots/news/kobert/42/source-encoder.pt
save pretrained model to: /content/snapshots/news/kobert/42/source-classifier.pt
=== Evaluating classifier for source domain ===
Avg Loss = 0.6908, Avg Accuracy = 0.5000
Avg Loss = 0.6870, Avg Accuracy = 0.5000
Avg Loss = 0.6936, Avg Accuracy = 0.5000
=== Training encoder for target domain ===
Epoch [01/03] Step [001/025]: acc=0.5000 g_loss=0.6834 d_loss=0.6933 kd_loss=0.0003
Epoch [01/03] Step [002/025]: acc=0.5000 g_loss=0.6855 d_loss=0.6932 kd_loss=0.0003
Epoch [01/03] Step [003/025]: acc=0.5000 g_loss=0.6889 d_loss=0.6932 kd_loss=0.0004
Epoch [01/03] Step [004/025]: acc=0.5000 g_loss=0.6939 d_loss=0.6932 kd_loss=0.0002
Epoch [01/03] Step [005/025]: acc=0.5000 g_loss=0.6961 d_loss=0.6932 kd_loss=0.0004
Epoch [01/03] Step [006/025]: acc=0.5000 g_loss=0.6980 d_loss=0.6931 kd_loss=0.0003
Epoch [01/03] Step [007/025]: acc=0.5000 g_loss=0.6979 d_loss=0.6932 kd_loss=0.0004
Epoch [01/03] Step [008/025]: acc=0.5000 g_loss=0.6972 d_loss=0.6930 kd_loss=0.0002
Epoch [01/03] Step [009/025]: acc=0.5000 g_loss=0.6957 d_loss=0.6931 kd_loss=0.0007
Epoch [01/03] Step [010/025]: acc=0.5000 g_loss=0.6939 d_loss=0.6931 kd_loss=0.0002
Epoch [01/03] Step [011/025]: acc=0.5000 g_loss=0.6924 d_loss=0.6930 kd_loss=0.0009
Epoch [01/03] Step [012/025]: acc=0.5000 g_loss=0.6853 d_loss=0.6929 kd_loss=0.0014
Epoch [01/03] Step [013/025]: acc=0.5000 g_loss=0.6772 d_loss=0.6932 kd_loss=0.0024
Epoch [01/03] Step [014/025]: acc=0.5000 g_loss=0.6755 d_loss=0.6935 kd_loss=0.0012
Epoch [01/03] Step [015/025]: acc=0.5000 g_loss=0.6763 d_loss=0.6931 kd_loss=0.0005
Epoch [01/03] Step [016/025]: acc=0.5000 g_loss=0.6744 d_loss=0.6934 kd_loss=0.0005
Epoch [01/03] Step [017/025]: acc=0.5000 g_loss=0.6754 d_loss=0.6932 kd_loss=0.0015
Epoch [01/03] Step [018/025]: acc=0.5000 g_loss=0.6771 d_loss=0.6929 kd_loss=0.0003
Epoch [01/03] Step [019/025]: acc=0.5000 g_loss=0.6778 d_loss=0.6933 kd_loss=0.0016
Epoch [01/03] Step [020/025]: acc=0.5000 g_loss=0.6809 d_loss=0.6935 kd_loss=0.0014
Epoch [01/03] Step [021/025]: acc=0.5000 g_loss=0.6818 d_loss=0.6936 kd_loss=0.0013
Epoch [01/03] Step [022/025]: acc=0.5000 g_loss=0.6836 d_loss=0.6933 kd_loss=0.0011
Epoch [01/03] Step [023/025]: acc=0.5000 g_loss=0.6867 d_loss=0.6931 kd_loss=0.0010
Epoch [01/03] Step [024/025]: acc=0.5000 g_loss=0.6884 d_loss=0.6930 kd_loss=0.0003
Epoch [01/03] Step [025/025]: acc=0.5000 g_loss=0.6884 d_loss=0.6933 kd_loss=0.0011
Avg Loss = 0.6931, Avg Accuracy = 0.5000
Epoch [02/03] Step [001/025]: acc=0.5000 g_loss=0.6881 d_loss=0.6932 kd_loss=0.0005
Epoch [02/03] Step [002/025]: acc=0.5000 g_loss=0.6892 d_loss=0.6932 kd_loss=0.0009
Epoch [02/03] Step [003/025]: acc=0.5000 g_loss=0.6900 d_loss=0.6932 kd_loss=0.0005
Epoch [02/03] Step [004/025]: acc=0.5000 g_loss=0.6909 d_loss=0.6931 kd_loss=0.0017
Epoch [02/03] Step [005/025]: acc=0.5000 g_loss=0.6912 d_loss=0.6931 kd_loss=0.0005
Epoch [02/03] Step [006/025]: acc=0.5000 g_loss=0.6915 d_loss=0.6932 kd_loss=0.0006
Epoch [02/03] Step [007/025]: acc=0.5000 g_loss=0.6917 d_loss=0.6932 kd_loss=0.0010
Epoch [02/03] Step [008/025]: acc=0.5000 g_loss=0.6920 d_loss=0.6932 kd_loss=0.0010
Epoch [02/03] Step [009/025]: acc=0.5000 g_loss=0.6920 d_loss=0.6932 kd_loss=0.0010
Epoch [02/03] Step [010/025]: acc=0.5000 g_loss=0.6921 d_loss=0.6931 kd_loss=0.0002
Epoch [02/03] Step [011/025]: acc=0.5000 g_loss=0.6916 d_loss=0.6932 kd_loss=0.0015
Epoch [02/03] Step [012/025]: acc=0.5000 g_loss=0.6914 d_loss=0.6931 kd_loss=0.0003
Epoch [02/03] Step [013/025]: acc=0.5000 g_loss=0.6907 d_loss=0.6931 kd_loss=0.0008
Epoch [02/03] Step [014/025]: acc=0.5000 g_loss=0.6904 d_loss=0.6931 kd_loss=0.0003
Epoch [02/03] Step [015/025]: acc=0.5000 g_loss=0.6902 d_loss=0.6932 kd_loss=0.0006
Epoch [02/03] Step [016/025]: acc=0.5000 g_loss=0.6923 d_loss=0.6931 kd_loss=0.0004
Epoch [02/03] Step [017/025]: acc=0.5000 g_loss=0.6922 d_loss=0.6932 kd_loss=0.0023
Epoch [02/03] Step [018/025]: acc=0.5000 g_loss=0.6926 d_loss=0.6931 kd_loss=0.0003
Epoch [02/03] Step [019/025]: acc=0.5000 g_loss=0.6943 d_loss=0.6931 kd_loss=0.0003
Epoch [02/03] Step [020/025]: acc=0.5000 g_loss=0.6927 d_loss=0.6931 kd_loss=0.0002
Epoch [02/03] Step [021/025]: acc=0.5000 g_loss=0.6909 d_loss=0.6932 kd_loss=0.0003
Epoch [02/03] Step [022/025]: acc=0.5000 g_loss=0.6912 d_loss=0.6932 kd_loss=0.0009
Epoch [02/03] Step [023/025]: acc=0.5000 g_loss=0.6914 d_loss=0.6932 kd_loss=0.0001
Epoch [02/03] Step [024/025]: acc=0.5000 g_loss=0.6911 d_loss=0.6932 kd_loss=0.0003
Epoch [02/03] Step [025/025]: acc=0.5000 g_loss=0.6913 d_loss=0.6932 kd_loss=0.0001
Avg Loss = 0.6950, Avg Accuracy = 0.5000
Epoch [03/03] Step [001/025]: acc=0.5000 g_loss=0.6907 d_loss=0.6932 kd_loss=0.0011
Epoch [03/03] Step [002/025]: acc=0.5000 g_loss=0.6908 d_loss=0.6931 kd_loss=0.0001
Epoch [03/03] Step [003/025]: acc=0.5000 g_loss=0.6905 d_loss=0.6931 kd_loss=0.0003
Epoch [03/03] Step [004/025]: acc=0.5000 g_loss=0.6903 d_loss=0.6932 kd_loss=0.0004
Epoch [03/03] Step [005/025]: acc=0.5000 g_loss=0.6897 d_loss=0.6932 kd_loss=0.0002
Epoch [03/03] Step [006/025]: acc=0.5000 g_loss=0.6897 d_loss=0.6932 kd_loss=0.0004
Epoch [03/03] Step [007/025]: acc=0.5000 g_loss=0.6896 d_loss=0.6931 kd_loss=0.0004
Epoch [03/03] Step [008/025]: acc=0.5000 g_loss=0.6902 d_loss=0.6931 kd_loss=0.0004
Epoch [03/03] Step [009/025]: acc=0.5000 g_loss=0.6905 d_loss=0.6932 kd_loss=0.0014
Epoch [03/03] Step [010/025]: acc=0.5000 g_loss=0.6910 d_loss=0.6932 kd_loss=0.0010
Epoch [03/03] Step [011/025]: acc=0.5000 g_loss=0.6916 d_loss=0.6931 kd_loss=0.0002
Epoch [03/03] Step [012/025]: acc=0.5000 g_loss=0.6920 d_loss=0.6931 kd_loss=0.0018
Epoch [03/03] Step [013/025]: acc=0.5000 g_loss=0.6927 d_loss=0.6931 kd_loss=0.0009
Epoch [03/03] Step [014/025]: acc=0.5000 g_loss=0.6932 d_loss=0.6931 kd_loss=0.0003
Epoch [03/03] Step [015/025]: acc=0.5000 g_loss=0.6939 d_loss=0.6931 kd_loss=0.0002
Epoch [03/03] Step [016/025]: acc=0.5000 g_loss=0.6950 d_loss=0.6932 kd_loss=0.0005
Epoch [03/03] Step [017/025]: acc=0.5000 g_loss=0.6934 d_loss=0.6931 kd_loss=0.0004
Epoch [03/03] Step [018/025]: acc=0.5000 g_loss=0.6930 d_loss=0.6932 kd_loss=0.0011
Epoch [03/03] Step [019/025]: acc=0.5000 g_loss=0.6940 d_loss=0.6932 kd_loss=0.0002
Epoch [03/03] Step [020/025]: acc=0.5000 g_loss=0.6929 d_loss=0.6932 kd_loss=0.0006
Epoch [03/03] Step [021/025]: acc=0.5000 g_loss=0.6925 d_loss=0.6931 kd_loss=0.0007
Epoch [03/03] Step [022/025]: acc=0.5000 g_loss=0.6927 d_loss=0.6931 kd_loss=0.0004
Epoch [03/03] Step [023/025]: acc=0.5000 g_loss=0.6929 d_loss=0.6931 kd_loss=0.0031
Epoch [03/03] Step [024/025]: acc=0.5000 g_loss=0.6938 d_loss=0.6931 kd_loss=0.0012
Epoch [03/03] Step [025/025]: acc=0.5000 g_loss=0.6907 d_loss=0.6931 kd_loss=0.0008
Avg Loss = 0.6928, Avg Accuracy = 0.5000
=== Evaluating classifier for encoded target domain ===
>>> source only <<<
Avg Loss = 0.6917, Avg Accuracy = 0.5000
>>> domain adaption <<<
Avg Loss = 0.6926, Avg Accuracy = 0.5000
